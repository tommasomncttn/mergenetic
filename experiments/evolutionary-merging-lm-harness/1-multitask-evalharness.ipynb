{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/leonardo/home/userexternal/tmencatt/miniconda3/envs/mergenetic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.operators.repair.rounding import RoundingRepair\n",
    "from pymoo.operators.sampling.rnd import IntegerRandomSampling\n",
    "\n",
    "from mergenetic import PROJECT_ROOT\n",
    "from mergenetic.evaluation import *\n",
    "from mergenetic.utils import *\n",
    "from mergenetic.merging.merger import *\n",
    "from mergenetic.merging import SlerpMerger, TiesDareMerger\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from mergenetic.optimization import MergingProblem\n",
    "from mergenetic.searcher import SearcherLMEval\n",
    "from dataclasses import dataclass\n",
    "import yaml\n",
    "import pickle\n",
    "from end2end_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Evolutionary Model Merging with Mergenetic\n",
    "\n",
    "In this notebook, we illustrate how to perform evolutionary merging of language models using the Mergenetic library for a **problem defined via lm-eval harness**. \n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "1. **Configuration Merging** - Define the configuration for the merging problem.\n",
    "1. **Data Loading** – Read the CSV dataset.\n",
    "2. **Anchor Extraction** – Randomly select anchor points.\n",
    "3. **Pre-Evaluation (Optional)** – Evaluate the base models (can be skipped).\n",
    "4. **Theta Retrieval** – Obtain the latent ability parameters.\n",
    "5. **Train/Test Split** – Separate the sampled anchors from the rest of the data.\n",
    "6. **Set Performance Estimation Parameters** – Unpack and configure the parameters for performance estimation.\n",
    "7. **Define the Merger** – Create the merging object (using SlerpMerger in this example).\n",
    "8. **Define the Optimization Problem** – Specify the problem instance that wraps the merger, evaluation data, and settings.\n",
    "9. **Define the Evolutionary Algorithm** – Set up a genetic algorithm (GA) with sampling, crossover, and mutation operators.\n",
    "10. **Run the Search** – Execute the evolutionary search and test the merged model.\n",
    "\n",
    "If you wish to run the same problem via a script, the code can be found at [this script](end2end.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration for Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This configuration file sets up the parameters for running the cross-lingual evolutionary merging process. The goal is to merge language models using evolutionary algorithms while controlling evaluation and performance estimation aspects. To do so we need to furnish a list of details, like the model that must be merged, the task on which they should be evaluated,... Most importantly, three types of parameters must be furnished:\n",
    "\n",
    "REQUIRED PARAMETERS:\n",
    "\n",
    "- **pop_size** (`int`):  \n",
    "  Specifies the size of the population in the evolutionary search (e.g., `25`).\n",
    "\n",
    "- **n_iter** (`int`):  \n",
    "  The number of iterations (or generations) to run the evolutionary algorithm (e.g., `7`).\n",
    "\n",
    "- **ft_model_paths** (`List[str]`):  \n",
    "  A list of fine-tuning model identifiers/paths. In this example, it includes:  \n",
    "  - `\"OpenLLM-Ro/RoMistral-7b-Instruct\"`  \n",
    "  - `\"meta-math/MetaMath-Mistral-7B\"`\n",
    "\n",
    "- **path_to_store_merged_model** (`str`):  \n",
    "  Path where the merged model(s) will be stored during the evolutionary search (e.g., `\"experiments/models/merged/\"`).\n",
    "\n",
    "- **path_to_store_yaml** (`str`):  \n",
    "  Directory path for saving YAML configuration files (e.g., `\"experiments/evolutionary-merging-lm-harness/romanian_math/\"`).\n",
    "\n",
    "- **dtype** (`str`):  \n",
    "  Data type used for model parameters; here it is `\"float16\"`.\n",
    "\n",
    "- **run_id** (`str`):  \n",
    "  A unique identifier for this run or experiment (e.g., `\"romanian_math_gmpirt\"`).\n",
    "\n",
    "- **bench** (`str`):  \n",
    "  Specifies the benchmark to use for evaluation (e.g., `\"gsm8k\"`). \n",
    "\n",
    "- **mode** (`str`):  \n",
    "  Indicates the merging mode (e.g., `\"gmpirt\"`).\n",
    "\n",
    "- **seed** (`int`):  \n",
    "  Random seed value to ensure reproducibility (e.g., `420`). &#128521;\n",
    "\n",
    "- **dataset_path** (`str`):  \n",
    "  File path to the dataset used for evaluation (e.g., `\"data/new_datasets/gsm8k/gsm8k_test_romanian.csv\"`).\n",
    "\n",
    "- **tasks** (`dict[str, str]`):  \n",
    "  The lm-eval harness task on which we wish to test the model:\n",
    "  - `sample`: `\"gsm8k-ro\"`  \n",
    "  - `test`: `\"gsm8k-ro\"`\n",
    "\n",
    "- **metric** (`str`):  \n",
    "  The evaluation metric to be used (e.g., `\"exact_match\"`) is the key of the dictionary that must be used to access the quantitive performance of the model in the task defined above from the dictionary returned by lm-eval harness.  \n",
    "  *Default is `\"acc\"` if not explicitly set.*\n",
    "\n",
    "\n",
    "OPTIONAL GENERIC PARAMETERS:\n",
    "\n",
    "- **device** (`str | None`):  \n",
    "  Specifies the computing device (e.g., `\"cuda:0\"` for GPU, or `None` for CPU).\n",
    "\n",
    "OPTIONAL EVALUATION PARAMETERS:\n",
    "\n",
    "- **ft_model_to_evaluate** (`List[str]`):  \n",
    "  List of fine-tuned models to be evaluated in order to compute the latent ability ot this endpoint models. These are the models that we are going to merge. Model should be passed here when we do not have already the correctness score to extract the thetas. In alternative, you could pass directly the correctness path in responses_path.\n",
    "\n",
    "- **eval_task** (`str`):  \n",
    "  Specifies the evaluation task for computing the correctness scores of the model that we will evaluate (e.g., `\"MATH\"`).\n",
    "\n",
    "- **output_path_evaluation** (`List[str]`):  \n",
    "  List of paths where evaluation outputs will be saved.\n",
    "\n",
    "- **eval_batch_size** (`int`):  \n",
    "  Batch size used during evaluation (e.g., `64`).\n",
    "\n",
    "OPTIONAL THETA ESTIMATION PARAMETERS:\n",
    "\n",
    "- **responses_path** (`List[str]`):  \n",
    "  List of paths to model responses. \n",
    "\n",
    "- **output_path_theta_estimation** (`List[str]`):  \n",
    "  List of paths to save theta estimation results. Example values include:  \n",
    "  - `\"experiments/evolutionary-merging-lm-harness/results/ft_ro_theta.pkl\"`  \n",
    "  - `\"experiments/evolutionary-merging-lm-harness/results/metamath_ro_theta.pkl\"`\n",
    "\n",
    "\n",
    "OPTIONAL FOR PROMPTING\n",
    "- **custom_prompt_template** (`str`):  \n",
    "  A custom prompt template that can be used in prompting the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the configuration\n",
    "config = ConfigCrossLingual(\n",
    "    pop_size=25,\n",
    "    n_iter=7,\n",
    "    ft_model_paths=[\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/models/base/OpenLLM-Ro/RoMistral-7b-Instruct\",\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/models/base/meta-math/MetaMath-Mistral-7B\",\n",
    "    ],\n",
    "    path_to_store_merged_model=\"/leonardo_scratch/large/userexternal/tmencatt\",\n",
    "    path_to_store_yaml=\"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/\",\n",
    "    dtype=\"float16\",\n",
    "    run_id=\"romanian_math_gmpirt\",\n",
    "    bench=\"gsm8k\",\n",
    "    mode=\"gmpirt\",\n",
    "    seed=420,\n",
    "    tasks={\"sample\": \"gsm8k-ro\", \"test\": \"gsm8k-ro\"},\n",
    "    metric=\"exact_match\",\n",
    "    dataset_path=\"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/data/new_datasets/gsm8k/gsm8k_test_romanian.csv\",\n",
    "    device=\"cuda:0\",\n",
    "    ft_model_to_evaluate=[\"\", \"\"],\n",
    "    eval_task=\"MATH\",\n",
    "    output_path_evaluation=[\"\", \"\"],\n",
    "    eval_batch_size=64,\n",
    "    responses_path=[\"\", \"\"],\n",
    "    output_path_theta_estimation=[\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/ft_ro_theta.pkl\",\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/metamath_ro_theta.pkl\",\n",
    "    ],\n",
    "    thetas_paths=[\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/ft_ro_theta.pkl\",\n",
    "        \"/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/metamath_ro_theta.pkl\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation configuration:  ConfigCrossLingual(pop_size=25, n_iter=7, ft_model_paths=['/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/models/base/OpenLLM-Ro/RoMistral-7b-Instruct', '/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/models/base/meta-math/MetaMath-Mistral-7B'], path_to_store_merged_model='/leonardo_scratch/large/userexternal/tmencatt', path_to_store_yaml='/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/', dtype='float16', run_id='romanian_math_gmpirt', dataset_path='/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/data/new_datasets/gsm8k/gsm8k_test_romanian.csv', thetas_paths=['/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/ft_ro_theta.pkl', '/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/metamath_ro_theta.pkl'], bench='gsm8k', mode='gmpirt', seed=420, tasks={'sample': 'gsm8k-ro', 'test': 'gsm8k-ro'}, metric='exact_match', device='cuda:0', ft_model_to_evaluate=['', ''], eval_task='MATH', output_path_evaluation=['', ''], eval_batch_size=64, responses_path=['', ''], output_path_theta_estimation=['/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/ft_ro_theta.pkl', '/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/results/metamath_ro_theta.pkl'], custom_prompt_template='')\n",
      "STEP 2 completed: Predictions obtained\n"
     ]
    }
   ],
   "source": [
    "# STEP 3. Get the responses of the base models\n",
    "predictions = evaluate_model(config)\n",
    "print(\"STEP 2 completed: Predictions obtained\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3 completed: Thetas obtained\n"
     ]
    }
   ],
   "source": [
    "# STEP 4. Get the thetas\n",
    "thetas = retrieve_thetas(config)\n",
    "print(\"STEP 3 completed: Thetas obtained\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4 completed: Accuracy estimation parameters set\n"
     ]
    }
   ],
   "source": [
    "# STEP 5. Unpack some parameters and set the accuracy estimation parameters\n",
    "pop_size = config.pop_size\n",
    "n_iter = config.n_iter\n",
    "run_id = config.run_id\n",
    "bench = config.bench\n",
    "mode = config.mode\n",
    "tasks = config.tasks\n",
    "metric = config.metric\n",
    "est_parameters = AccuracyEstimationLMEval(\n",
    "    thetas=thetas, bench=bench, mode=mode, tasks=tasks, metric=metric\n",
    ")\n",
    "print(\"STEP 4 completed: Accuracy estimation parameters set\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 completed: Merger defined\n"
     ]
    }
   ],
   "source": [
    "# STEP 7. Define the merger\n",
    "\n",
    "merger = SlerpMerger(\n",
    "    run_id=run_id,\n",
    "    path_to_base_model=config.ft_model_paths[0],\n",
    "    path_to_model_1=config.ft_model_paths[1],\n",
    "    path_to_store_yaml=config.path_to_store_yaml,\n",
    "    path_to_store_merged_model=config.path_to_store_merged_model,\n",
    "    dtype=config.dtype,\n",
    "    layer_range_base_model=[0, 32],\n",
    "    layer_range_model_1=[0, 32],\n",
    ")\n",
    "\n",
    "\n",
    "if config.device:\n",
    "    device = config.device\n",
    "else:\n",
    "    device = \"cuda\"\n",
    "print(\"STEP 5 completed: Merger defined\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8 completed: Problem defined\n"
     ]
    }
   ],
   "source": [
    "# STEP 8. Define the problem\n",
    "problem = AnchoredCrossLingualMathProblem(\n",
    "    merger,\n",
    "    evolve_task=tasks[\"sample\"],\n",
    "    test_task=tasks[\"test\"],\n",
    "    n_var=11,\n",
    "    n_obj=1,\n",
    "    n_eq_constr=0,\n",
    "    n_ieq_constr=0,\n",
    "    discrete=True,\n",
    "    eval_batch_size=64,\n",
    "    device=device,\n",
    ")\n",
    "problem.set_est_parameters(est_parameters)\n",
    "print(\"STEP 8 completed: Problem defined\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 9 completed: Algorithm defined\n"
     ]
    }
   ],
   "source": [
    "# STEP 9. Define the algorithm\n",
    "algorithm = GA(\n",
    "    pop_size=pop_size,\n",
    "    sampling=IntegerRandomSampling(),\n",
    "    crossover=SBX(),\n",
    "    mutation=PM(),\n",
    "    eliminate_duplicates=True,\n",
    ")\n",
    "print(\"STEP 9 completed: Algorithm defined\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml configuration file created at /leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/romanian_math/romanian_math_gmpirt/config.yaml\n",
      "The folder does not exist: /leonardo_scratch/large/userexternal/tmencatt/romanian_math_gmpirt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|██████████| 2/2 [00:00<00:00, 89.70it/s]\n",
      "Executing graph:   0%|          | 3/1457 [00:00<00:51, 28.01it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for lm_head.weight\n",
      "Executing graph:   0%|          | 6/1457 [00:05<27:37,  1.14s/it]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for model.embed_tokens.weight\n",
      "Executing graph: 100%|██████████| 1457/1457 [03:38<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /leonardo_scratch/large/userexternal/tmencatt/romanian_math_gmpirt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gsm8k-ro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m result_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments/evolutionary-merging-lm-harness\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m searcher \u001b[38;5;241m=\u001b[39m SearcherLMEval(problem, algorithm, result_path, n_iter, run_id \u001b[38;5;241m=\u001b[39m run_id, seed\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/src/mergenetic/searcher/searcher.py:146\u001b[0m, in \u001b[0;36mSearcherLMEval.search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the search algorithm and saves the results to a CSV file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_iter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest solution found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/optimize.py:67\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     algorithm\u001b[38;5;241m.\u001b[39msetup(problem, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# actually execute the algorithm\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[1;32m     70\u001b[0m res\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m=\u001b[39m algorithm\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/algorithm.py:138\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/algorithm.py:158\u001b[0m, in \u001b[0;36mAlgorithm.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# call the advance with them after evaluation\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infills \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(infills\u001b[38;5;241m=\u001b[39minfills)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# if the algorithm does not follow the infill-advance scheme just call advance\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/evaluator.py:69\u001b[0m, in \u001b[0;36mEvaluator.eval\u001b[0;34m(self, problem, pop, skip_already_evaluated, evaluate_values_of, count_evals, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# evaluate the solutions (if there are any)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(I) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# do the actual evaluation - call the sub-function to set the corresponding values to the population\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# update the function evaluation counter\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_evals:\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/evaluator.py:90\u001b[0m, in \u001b[0;36mEvaluator._eval\u001b[0;34m(self, problem, pop, evaluate_values_of, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m X \u001b[38;5;241m=\u001b[39m pop\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# call the problem to evaluate the solutions\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# for each of the attributes set it to the problem\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/problem.py:257\u001b[0m, in \u001b[0;36mProblem.evaluate\u001b[0;34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     only_single_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np\u001b[38;5;241m.\u001b[39mndarray))\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m _out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _out\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/problem.py:297\u001b[0m, in \u001b[0;36mProblem.do\u001b[0;34m(self, X, return_values_of, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# do the function evaluation\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melementwise:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_elementwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_vectorized(X, out, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/problem.py:315\u001b[0m, in \u001b[0;36mProblem._evaluate_elementwise\u001b[0;34m(self, X, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melementwise_func(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# execute the runner\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m elems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melementwise_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# for each evaluation call\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m elems:\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# for each key stored for this evaluation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/problem.py:32\u001b[0m, in \u001b[0;36mLoopedElementwiseEvaluation.__call__\u001b[0;34m(self, f, X)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, X):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/pymoo/core/problem.py:25\u001b[0m, in \u001b[0;36mElementwiseEvaluationFunction.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/src/mergenetic/optimization/merging_problem.py:643\u001b[0m, in \u001b[0;36mMergingLMEValProblem._evaluate\u001b[0;34m(self, x, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_genotype_to_phenotype(path_to_model)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# Compute the evaluation metrics\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m f, description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_4_genotype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Assign the computed metrics to the output dictionary\u001b[39;00m\n\u001b[1;32m    646\u001b[0m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m f\n",
      "File \u001b[0;32m/leonardo_work/IscrC_MGNTC/tmencatt/mergenetic/experiments/evolutionary-merging-lm-harness/end2end_utils.py:388\u001b[0m, in \u001b[0;36mAnchoredCrossLingualMathProblem.metrics_4_genotype\u001b[0;34m(self, model, task, get_also_series, on_full_df)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_on_gpu: device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 388\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your hardware capabilities\u001b[39;49;00m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_responses\u001b[39m(results):\n\u001b[1;32m    396\u001b[0m     answers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/utils.py:422\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m     )\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/evaluator.py:240\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, max_batch_size, device, use_cache, cache_requests, rewrite_requests_cache, delete_requests_cache, limit, bootstrap_iters, check_integrity, write_out, log_samples, evaluation_tracker, system_instruction, apply_chat_template, fewshot_as_multiturn, gen_kwargs, task_manager, verbostiy, predict_only, random_seed, numpy_random_seed, torch_random_seed, fewshot_random_seed, confirm_run_unsafe_code)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     task_manager \u001b[38;5;241m=\u001b[39m TaskManager()\n\u001b[0;32m--> 240\u001b[0m task_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_task_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# helper function to recursively apply config overrides to leaf subtasks, skipping their constituent groups.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# (setting of num_fewshot ; bypassing metric calculation ; setting fewshot seed)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_adjust_config\u001b[39m(task_dict):\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/tasks/__init__.py:619\u001b[0m, in \u001b[0;36mget_task_dict\u001b[0;34m(task_name_list, task_manager)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m         task_manager \u001b[38;5;241m=\u001b[39m TaskManager()\n\u001b[0;32m--> 619\u001b[0m     task_name_from_string_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtask_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_task_or_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring_task_name_list\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_element \u001b[38;5;129;01min\u001b[39;00m others_task_name_list:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task_element, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/tasks/__init__.py:415\u001b[0m, in \u001b[0;36mTaskManager.load_task_or_group\u001b[0;34m(self, task_list)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task_list, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    412\u001b[0m     task_list \u001b[38;5;241m=\u001b[39m [task_list]\n\u001b[1;32m    414\u001b[0m all_loaded_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m--> 415\u001b[0m     \u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChainMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_individual_task_or_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_loaded_tasks\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/tasks/__init__.py:317\u001b[0m, in \u001b[0;36mTaskManager._load_individual_task_or_group\u001b[0;34m(self, name_or_config, parent_name, update_config)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_task(task_config, task\u001b[38;5;241m=\u001b[39mname_or_config)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     subtask_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tasklist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subtask_list \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    319\u001b[0m         group_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config(name_or_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/mergenetic/lib/python3.12/site-packages/lm_eval/tasks/__init__.py:237\u001b[0m, in \u001b[0;36mTaskManager._get_tasklist\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_is_task(name):\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gsm8k-ro'"
     ]
    }
   ],
   "source": [
    "# STEP 10. Define the searcher and run it\n",
    "result_path = \"experiments/evolutionary-merging-lm-harness\"\n",
    "searcher = SearcherLMEval(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    result_path,\n",
    "    n_iter,\n",
    "    run_id=run_id,\n",
    "    seed=config.seed,\n",
    "    verbose=False,\n",
    ")\n",
    "searcher.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mergenetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
