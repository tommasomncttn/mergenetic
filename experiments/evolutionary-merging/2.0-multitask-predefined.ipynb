{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2112d4f0-7bb2-4d5f-9f7d-c9122ad24b95",
   "metadata": {},
   "source": [
    "# Tutorial: Evolutionary Model Merging with Mergenetic\n",
    "\n",
    "In this notebook, we illustrate how to perform evolutionary merging of language models using the Mergenetic library. We go step by step, following the logic of the script you provided. This tutorial is focused on a cross-lingual math merging experiment. \n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "1. **Data Loading** – Read the CSV dataset.\n",
    "2. **Anchor Extraction** – Randomly select anchor points.\n",
    "3. **Pre-Evaluation (Optional)** – Evaluate the base models (can be skipped).\n",
    "4. **Theta Retrieval** – Obtain the latent ability parameters.\n",
    "5. **Train/Test Split** – Separate the sampled anchors from the rest of the data.\n",
    "6. **Set Performance Estimation Parameters** – Unpack and configure the parameters for performance estimation.\n",
    "7. **Define the Merger** – Create the merging object (using SlerpMerger in this example).\n",
    "8. **Define the Optimization Problem** – Specify the problem instance that wraps the merger, evaluation data, and settings.\n",
    "9. **Define the Evolutionary Algorithm** – Set up a genetic algorithm (GA) with sampling, crossover, and mutation operators.\n",
    "10. **Run the Search** – Execute the evolutionary search and test the merged model.\n",
    "\n",
    "Let’s begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4f3d5-44c7-41e6-b626-d2d9e2cf42aa",
   "metadata": {},
   "source": [
    "# Step 0: Import required modules\n",
    "from pymoo.operators.sampling.rnd import IntegerRandomSampling\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from mergenetic.searcher import Searcher\n",
    "from mergenetic.merging import SlerpMerger\n",
    "from mergenetic.optimization.predefined_problems import (\n",
    "    CrossLingualMathProblem,\n",
    "    ConfigPE,\n",
    ")\n",
    "from end2end_utils import evaluate_model, retrieve_thetas, ConfigCrossLingual\n",
    "\n",
    "# For this tutorial, we assume that the required packages and modules are installed and that the\n",
    "# 'mergenetic' library along with 'end2end_utils' is available in your PYTHONPATH.\n",
    "\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27b0e-c008-4f7e-a9e4-65a2e0e8408a",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "In a regular script you might use command-line arguments and load a YAML file for configuration. In this tutorial we directly define a sample configuration (an instance of `ConfigCrossLingual`). Adjust the values as needed for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcefc4c-22e9-44b3-9c4f-2a7f83e77e95",
   "metadata": {},
   "source": [
    "# Step 1: Define a sample configuration\n",
    "# In practice, you could load this from a YAML file.\n",
    "config_dict = {\n",
    "    \"dataset_path\": \"data/cross_lingual_math.csv\",  # path to your CSV file\n",
    "    \"n_clusters\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"pop_size\": 25,\n",
    "    \"n_iter\": 7,\n",
    "    \"run_id\": \"cross_lingual_math_experiment\",\n",
    "    \"bench\": \"math\",\n",
    "    \"mode\": \"gmpirt\",\n",
    "    \"correctness_metric\": \"accuracy\",\n",
    "    \"ft_model_paths\": [\"base_model_path\", \"finetuned_model_path\"],\n",
    "    \"path_to_store_yaml\": \"results/configs/\",\n",
    "    \"path_to_store_merged_model\": \"results/merged_models/\",\n",
    "    \"dtype\": \"fp16\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"eval_batch_size\": 64,\n",
    "    \"lang_id\": \"en\",\n",
    "    \"no_preeval\": False,\n",
    "    # additional parameters as needed\n",
    "}\n",
    "\n",
    "# Create a ConfigCrossLingual object from the dictionary\n",
    "config = ConfigCrossLingual(**config_dict)\n",
    "\n",
    "print(f\"Configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af8e7f-2ed0-42d6-9bb7-474823b0a9bb",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "We load our dataset using `pandas`. Make sure your CSV file is in the correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0e9c7-6e53-4fa4-9e42-7e8f8584075d",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(config.dataset_path)\n",
    "print(\"STEP 1 completed: Data loaded\")\n",
    "print(f\"Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06b8bd-7d9a-4c70-9a60-6a7bde1e3a7e",
   "metadata": {},
   "source": [
    "## Step 2: Extract Random Anchor Points\n",
    "\n",
    "We randomly select a set of anchor points from the dataset. These anchors will be used later in the evaluation and performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a241a1-6a7f-4a2c-b5d4-0c630b7f6e3a",
   "metadata": {},
   "source": [
    "# Extract random anchor points\n",
    "anchors = df.sample(n=config.n_clusters, random_state=config.seed).index\n",
    "anchors_weights = np.ones(len(anchors)) / len(anchors)\n",
    "\n",
    "print(\"STEP 2 completed: Anchors extracted\")\n",
    "print(f\"Anchors: {anchors.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae8c5c-6e33-4af0-b22f-d8462c5b2a7d",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Pre-Evaluation of Base Models\n",
    "\n",
    "If your configuration requires it (i.e. if `no_preeval` is false), this step evaluates the base models on the dataset. In this example, we call `evaluate_model()`. You can skip this step if the pre-evaluation is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63f4df-5a4a-4f08-a7e0-4e18666b13b4",
   "metadata": {},
   "source": [
    "if config.no_preeval:\n",
    "    print(\"STEP 3: Skipping the pre-evaluation step.\")\n",
    "else:\n",
    "    _ = evaluate_model(config)\n",
    "    print(\"STEP 3 completed: Predictions obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5a2d2-34e0-4a63-a9bd-2549f8c1afc2",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve the Thetas\n",
    "\n",
    "Using the provided function `retrieve_thetas()`, we extract the latent ability parameters (thetas) that are used later in performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c227e-7012-4e51-95af-4071748c6811",
   "metadata": {},
   "source": [
    "# Retrieve the thetas\n",
    "thetas = retrieve_thetas(config)\n",
    "print(\"STEP 4 completed: Thetas obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e16ab-9037-4883-b10b-89d321c2b5e3",
   "metadata": {},
   "source": [
    "## Step 5: Extract Samples and Define Test Set\n",
    "\n",
    "We use the anchor indices to form a sampled dataset (for evaluation during search) and create a test set by dropping these anchor points from the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733ec42-3c78-4d26-9c81-c98d730a4f35",
   "metadata": {},
   "source": [
    "# Create sampled and test sets\n",
    "sampled_df = df.loc[anchors].copy()\n",
    "test_df = df.drop(anchors)\n",
    "\n",
    "print(\"STEP 5 completed: Samples extracted\")\n",
    "print(f\"Sampled data shape: {sampled_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac3385-1e3b-4d4e-8fdd-34f5c1e4f2e5",
   "metadata": {},
   "source": [
    "## Step 6: Set Performance Estimation Parameters\n",
    "\n",
    "We unpack some parameters from the configuration and create an instance of `ConfigPE` (performance estimation configuration) with the retrieved thetas, anchor weights, and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7a503-2e80-4c74-91d1-4eab236be895",
   "metadata": {},
   "source": [
    "# Unpack parameters and define performance estimation configuration\n",
    "pop_size = config.pop_size\n",
    "n_iter = config.n_iter\n",
    "run_id = config.run_id\n",
    "bench = config.bench\n",
    "mode = config.mode\n",
    "\n",
    "est_parameters = ConfigPE(\n",
    "    thetas=thetas,\n",
    "    weights=anchors_weights,\n",
    "    sample_ids=anchors,\n",
    "    bench=bench,\n",
    "    mode=mode,\n",
    "    correct_metric=config.correctness_metric,\n",
    ")\n",
    "\n",
    "print(\"STEP 6 completed: Performance estimation parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c482-d75c-453e-8c24-cd6e705d823d",
   "metadata": {},
   "source": [
    "## Step 7: Define the Merger\n",
    "\n",
    "We now create a merger instance. In this example we use the `SlerpMerger`. The merger is initialized with the base model and one finetuned model (from the list in `ft_model_paths`), as well as other parameters such as the layer ranges and where to store the YAML configuration and merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b8d9f-0422-4d9c-936b-5142f78f22cf",
   "metadata": {},
   "source": [
    "# Define the merger\n",
    "merger = SlerpMerger(\n",
    "    run_id=run_id,\n",
    "    path_to_base_model=config.ft_model_paths[0],\n",
    "    path_to_model_1=config.ft_model_paths[1],\n",
    "    path_to_store_yaml=config.path_to_store_yaml,\n",
    "    path_to_store_merged_model=config.path_to_store_merged_model,\n",
    "    dtype=config.dtype,\n",
    "    layer_range_base_model=[0, 32],\n",
    "    layer_range_model_1=[0, 32],\n",
    ")\n",
    "\n",
    "device = config.device if config.device else \"cuda\"\n",
    "print(\"STEP 7 completed: Merger defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85c88e-dc2a-44eb-a6fd-23ef4647f006",
   "metadata": {},
   "source": [
    "## Step 8: Define the Optimization Problem\n",
    "\n",
    "We wrap the merging operation in an optimization problem. Here, `CrossLingualMathProblem` is used, which takes the merger, the test dataset, the sampled data, and additional configuration parameters such as the language id, performance estimation parameters, device, number of variables, objectives, and evaluation batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c1b1d-6a55-4b1c-b43a-d2ad1e1a2a9e",
   "metadata": {},
   "source": [
    "# Define the problem\n",
    "problem = CrossLingualMathProblem(\n",
    "    merger,\n",
    "    test_df=test_df,\n",
    "    search_df=sampled_df,\n",
    "    lang_id=config.lang_id,\n",
    "    conf_pe=est_parameters,\n",
    "    device=device,\n",
    "    n_var=11,\n",
    "    n_obj=1,\n",
    "    n_eq_constr=0,\n",
    "    n_ieq_constr=0,\n",
    "    discrete=True,\n",
    "    eval_batch_size=config.eval_batch_size,\n",
    ")\n",
    "\n",
    "print(\"STEP 8 completed: Problem defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5ac6d-cc4c-4180-b425-75a9b5b32af8",
   "metadata": {},
   "source": [
    "## Step 9: Define the Evolutionary Algorithm\n",
    "\n",
    "We define a genetic algorithm (GA) for our optimization using a random integer sampling method, SBX crossover, and polynomial mutation. The algorithm is configured with a specified population size and duplicate elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a338fa-7e31-4a2b-b4b8-423d44d4e4e0",
   "metadata": {},
   "source": [
    "# Define the evolutionary algorithm\n",
    "algorithm = GA(\n",
    "    pop_size=pop_size,\n",
    "    sampling=IntegerRandomSampling(),\n",
    "    crossover=SBX(),\n",
    "    mutation=PM(),\n",
    "    eliminate_duplicates=True,\n",
    ")\n",
    "\n",
    "print(\"STEP 9 completed: Algorithm defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c41dd-640e-4256-bc05-5c06c70f8a3f",
   "metadata": {},
   "source": [
    "## Step 10: Run the Evolutionary Search\n",
    "\n",
    "Finally, we create a `Searcher` object with the problem, algorithm, and results path. Then we run the search process and test the best merged model found.\n",
    "\n",
    "This step carries out the evolutionary search over the specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c7d7e-7f3e-4f4c-8e73-f1d6b89c7c1a",
   "metadata": {},
   "source": [
    "# Define the searcher and run the evolutionary search\n",
    "searcher = Searcher(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    config.path_to_store_merged_model,\n",
    "    n_iter,\n",
    "    run_id=run_id,\n",
    "    seed=config.seed,\n",
    "    verbose=False,\n",
    ")\n",
    "searcher.search()\n",
    "searcher.test()\n",
    "\n",
    "print(\"STEP 10 completed: Search finished and testing done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7ddbe-3c5e-4a21-98be-6d3a7f85eac0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we walked through the complete process of setting up and executing an evolutionary merging experiment using Mergenetic. \n",
    "\n",
    "You can now modify the configuration, change the merging method or optimization parameters, and run your own experiments.\n",
    "\n",
    "**Happy merging!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
